# -*- coding: utf-8 -*-

'''
perfdata is a module for parsing and writing performance data in the perfdata
format.

For example::

    from __future__ import print_function
    import json
    import perfdata
    import sys

    with open('myfile.pdj') as file:
        pdj = json.load(file)
        pds = [pdj] if isinstance(pdj, dict) else pdj

        for pd in pds:
            try:
                pd = perfdata.Container(pd)
            except perfdata.ValidationError as e:
                print('invalid perfdata: ' + e.message, file=sys.stderr)
                sys.exit(1)

            print(pd.name + ':')
            for measurement in pd:
                print('{}: {}'.format(
                        measurement.metric,
                        measurement.value or measurement.mean))
'''

from __future__ import absolute_import, division, print_function
from __future__ import unicode_literals

__author__ = 'Apple Inc.'
__copyright__ = 'Copyright (c) 2017-2019 Apple Inc.'
__version__ = '2.0.0'

import datetime
import functools
import getpass
import io
import json
import math
import numbers
import os
import re
import six
import subprocess
import sys
import tarfile
import time
import zipfile

# Use new-style classes in Python 2.
__metaclass__ = type

# 2 + 3
retype = None
if sys.version_info[0] >= 3:
    from collections.abc import Iterable
    from typing.re import Pattern
    unicode = str
    retype = Pattern
    raw_input = input
else:
    from collections import Iterable
    retype = re._pattern_type
    json.decoder.JSONDecodeError = ValueError
    io.IOBase = file  # noqa: F821

# Without recertifi, certificate verification against BATS and Splunk will
# fail.
enable_cert_verify = False
try:
    import recertifi  # noqa: F401
    enable_cert_verify = True
except ImportError:
    pass


valid_schema_re = re.compile('https://perfdata.apple.com/schema/')


class ValidationError(Exception):
    def __init__(self, message):
        self.message = message

    def __str__(self):
        return str(self.message)


@functools.total_ordering
class Container():
    '''
    Containers parse perfdata and iterate over any measurements within.
    '''

    def __init__(self, input):
        '''
        The object can read existing perfdata JSON from either a file,
        string, or dictionary.
        '''
        pd = None
        try:
            if hasattr(input, 'read'):
                pd = json.load(input)
            elif isinstance(input, (str, unicode)):
                pd = json.loads(input)
        except ValueError as e:
            raise ValidationError('invalid JSON: ' + e.message)

        if not pd:
            if isinstance(input, dict):
                pd = input
            else:
                raise TypeError('perfdata must be file, string, or dictionary')

        if 'data' not in pd:
            pd['data'] = [[]]
        self.pd = pd
        if 'labels' in self.pd:
            for key, val in six.iteritems(self.pd.labels):
                self.pd.labels[key] = set([val])

        # Handle legacy perfdata files, which must contain the `bats_test_name`
        # key at the top-level.
        if 'bats_test_name' in self.pd:
            try:
                self.pd = self._convert_legacy(self.pd)
            except ValidationError as e:
                raise ValidationError(
                        'failed to convert from legacy format: ' + e.message)
            except Exception as e:
                raise ValidationError(
                        'failed to convert from legacy format: ' + str(e))
        elif '$schema' not in self.pd:
            raise ValidationError('no $schema field, unknown format')
        else:
            schema = self.pd['$schema']
            if not valid_schema_re.match(schema):
                raise ValidationError('unknown schema: ' + schema)

    def _getfield(self, key):
        '''
        Avoid KeyErrors and route fields to the `pd` instance variable.
        '''
        return self.pd.get(key, None)

    @staticmethod
    def _convert_legacy(legacy_pd):
        '''
        Convert a legacy perfdata dictionary to a normal perfdata dictionary.
        '''

        pd = {
            'name': legacy_pd['bats_test_name'],
            'version': 1,
        }
        if 'configuration' in legacy_pd:
            pd['configuration'] = legacy_pd['configuration']
        else:
            pd['configuration'] = {}

        if 'measurements' not in legacy_pd:
            raise ValidationError('missing required measurements key')

        # Collect all of the legacy data for each measurement into groups.
        groups = []
        for meas_name, meas in six.iteritems(legacy_pd['measurements']):
            for key in ('names', 'units'):
                if key not in meas:
                    raise ValidationError(
                            'measurement {} missing required {} key'.format(
                                    meas_name, key))

            for i, metric in enumerate(meas['names']):
                # Each name in a measurement corresponds to a metric, with the
                # measurement name stored as a variable (since it might be
                # significant).
                #
                # Use a template dictionary that's copied for each measurement.
                template = {
                    'metric': metric,
                    'unit': meas['units'][i],
                    'variables': {'measurement': meas_name}
                }

                if 'data' in meas and len(meas['data']) > 0:
                    # Handle measurements where all of the data is available by
                    # looping through each element of the data field, getting
                    # the value for the current name at index `i`.
                    if isinstance(meas['data'][0], list):
                        for d in meas['data']:
                            # Handle broken perfdata with empty data arrays.
                            if i >= len(d):
                                continue
                            m = template.copy()
                            m['value'] = float(d[i])
                            groups.append(m)
                    else:
                        m = template.copy()
                        m['value'] = float(meas['data'][i])
                        groups.append(m)
                elif 'samples' in meas:
                    # Handle measurements with summary statistics.
                    m = template.copy()

                    m['samples'] = meas['samples'][i]
                    # Map legacy statistic names to normal ones.
                    convert_stats = [('means', 'mean'),
                                     ('standard_deviations', 'std_dev'),
                                     ('medians', 'median'),
                                     ('minimums', 'minimum'),
                                     ('maximums', 'maximum')]
                    for stat_key, stat_name in convert_stats:
                        if stat_key in meas:
                            m[stat_name] = meas[stat_key][i]

                    if 'quantiles' in meas:
                        percentiles = []
                        for q_str, value in six.iteritems(
                                meas['quantiles'][i]):
                            # Legacy's quantiles are our percentiles.
                            percentile = {'%': int(q_str), 'value': value}
                            percentiles.append(percentile)
                        m['percentiles'] = percentiles

                    groups.append(m)

        pd['data'] = [groups]
        if 'description' in legacy_pd:
            pd['description'] = legacy_pd['description']

        return pd

    @property
    def name(self):
        """Get the perfdata's name."""
        return self._getfield('name')

    @property
    def version(self):
        """Get the perfdata's version."""
        return self._getfield('version')

    @property
    def project(self):
        """Get the perfdata's project."""
        try:
            return self.name[:self.name.index('.')]
        except ValueError:
            return None

    @property
    def size(self):
        """Get the perfdata's size."""
        return len(json.dumps(self.pd))

    @property
    def configuration(self):
        """Get a dictionary of the perfdata's configuration."""
        return self._getfield('configuration')

    @property
    def extensions(self):
        """
        Get a dictionary of the perfdata's extensions, with each namespace as a
        top-level field.
        """
        return self._getfield('extensions')

    @property
    def notes(self):
        """Get the perfdata's notes."""
        return self._getfield('notes')

    @property
    def generator(self):
        """Get the perfdata's generator."""
        return self._getfield('generator')

    @property
    def larger_better(self):
        """
        Get whether larger values are better by default for this perfdata.
        """
        return self.pd.get('larger_better', False)

    def comparable(self, other):
        """
        This returns True if measurements from the two perfdata objects
        are compatible and can be compared to each other, based on the name and
        version of the perfdata.
        """
        return self.name == other.name and self.version == other.version

    @property
    def variables(self):
        """
        Get the top-level variables for this perfdata, that are applied to all
        measurements.
        """
        return self.pd.get('variables', dict())

    @property
    def labels(self):
        """
        Get the top-level labels for this perfdata, that are applied to all
        measurements.
        """
        return self.pd.get('labels', dict())

    @property
    def variable_names(self):
        """Get all unique variable names present in this perfdata."""
        vnames = set(self.variables.keys())
        for m in self.measurements():
            vars = m.variables
            if vars:
                keysset = set(vars.keys())
                vnames = vnames.union(keysset)
        return vnames

    # Iteration through the measurements is handled below by yielding a
    # Container.

    def measurements(self, filter=None, context=True):
        """
        Iterate through the measurements present in the container, taking an
        optional `filter`.  The filter is either a string metric filter or a
        callable (e.g. a function or lambda) that takes the measurement and
        returns True if the measurement should be included in the iteration
        and False to hide the measurement.
        """
        if 'data' not in self.pd:
            return
        metric = None
        variables = None
        filter_fn = None
        filter_regex = None
        if filter:
            if isinstance(filter, str):
                if filter.startswith(self.name):
                    filter = filter[len(self.name) + 1:]
                metric, variables = Measurement.parse_filter(filter)
            elif isinstance(filter, retype):
                filter_regex = filter
            elif callable(filter):
                filter_fn = filter
            else:
                raise TypeError('filter should be a string or callable')

        groupoff = 0
        for i_group, group in enumerate(self.pd['data']):
            if not group:
                continue
            if 'values' in group[0]:
                nvals = len(group[0]['values'])
                for curval in range(0, nvals):
                    for meas in group:
                        measurement = Measurement(self)
                        measurement.reset(
                                meas, i_group + groupoff + curval, curval)
                        if measurement.context and not context:
                            continue
                        if measurement.filter(
                                metric=metric, variables=variables,
                                regex=filter_regex, function=filter_fn):
                            yield measurement
                groupoff += nvals
            else:
                for meas in group:
                    measurement = Measurement(self)
                    measurement.reset(meas, i_group + groupoff)
                    if measurement.context and not context:
                        continue
                    if measurement.filter(
                            metric=metric, variables=variables,
                            regex=filter_regex, function=filter_fn):
                        yield measurement

    def aggregate_measurements(
            self, filter=None, only_variables=[], ignoring_variables=[],
            save_values=False, context=False):
        aggregated = {}
        i = 0
        for m in self.measurements(filter, context=context):
            filtstr = m.get_filter(
                    only_variables=only_variables,
                    ignoring_variables=ignoring_variables)

            if filtstr not in aggregated:
                meas = {
                    'metric': m.metric,
                    'unit': m.unit,
                    'variables': m.variables,
                    'larger_better': m.larger_better,
                    'labels': m.labels,
                    'tags': set(m._tags),
                }
                for var in ignoring_variables:
                    if var in meas['variables']:
                        del meas['variables'][var]

                aggregated[filtstr] = i, meas.copy(), Stats(), []
                i += 1
            else:
                _, agg_meas, _, _ = aggregated[filtstr]
                agg_lbls = agg_meas['labels']
                for lbl, val in six.iteritems(m.labels):
                    if lbl not in agg_lbls:
                        agg_lbls[lbl] = val
                    else:
                        agg_lbls[lbl] = agg_lbls[lbl].union(val)

                agg_meas['labels'] = agg_lbls

                tags = m._tags
                if tags:
                    agg_meas['tags'] = agg_meas['tags'].union(tags)

            _, _, stats, values = aggregated[filtstr]
            # TODO check that the units match

            value = m.value
            if value is not None:
                if save_values:
                    values.append(value)
                stats.update(value)
            else:
                stats.update_stats(m.mean, m.std_dev, m.samples, m.median)

        measurements = []
        for i, am, stats, values in aggregated.values():
            mean, std_dev, samples, median, min, max = stats.get_stats()
            am['mean'] = mean
            am['std_dev'] = std_dev
            am['samples'] = samples
            am['median'] = median
            am['minimum'] = min
            am['maximum'] = max
            am['labels_parsed'] = am['labels']
            am['tags'] = am['tags']

            am['aggregate_values'] = values if values else None

            measurements.append((i, Measurement(pd=self, measurement=am)))

        measurements.sort(key=lambda k: k[0])
        for _, m in measurements:
            yield m

    def aggregate_matching_measurements(
            self, other_pd, filter=None, only_variables=[],
            ignoring_variables=[], save_values=False, context=False):
        other_ams = {}
        for am in other_pd.aggregate_measurements(
                filter=filter, only_variables=only_variables,
                ignoring_variables=ignoring_variables,
                save_values=save_values, context=context):
            filt = am.get_filter(ignoring_variables=ignoring_variables)
            other_ams[filt] = am

        for am in self.aggregate_measurements(
                filter=filter, only_variables=only_variables,
                ignoring_variables=ignoring_variables,
                save_values=save_values, context=context):
            filt = am.get_filter(ignoring_variables=ignoring_variables)
            if filt in other_ams:
                yield am, other_ams[filt]

    def concatenate(self, other_pd, strict=False):
        if not self.comparable(other_pd):
            raise RuntimeError('Containers are not comparable')

        # TODO strict, special errors for concatenate
        self.pd['data'].extend(other_pd.pd['data'])

    def __iter__(self):
        """
        Iterating through the Container object itself generates each
        measurement.
        """
        for m in self.measurements():
            yield m

    def __lt__(self, other):
        return self.name < other.name or self.version < other.version

    def __eq__(self, other):
        return self.name == other.name and self.version == other.version

    def __repr__(self):
        desc = '{} v{}'.format(self.name, self.version)
        build = self.configuration.get('build', None)
        device = self.configuration.get('device_type', None)
        if build and device:
            desc += ' ({} {})'.format(build, device)
        return desc


@functools.total_ordering
class Measurement(object):
    """
    A Measurement is generated when iterating over a Container object.  It
    always contains `group`, `metric`, `unit`, and `larger_better`
    properties.  It may not have a `value` property if the measurement is a
    summary -- in that case use the `statistics` property to get a
    dictionary with the statistics that are available about the
    measurement.

    The `variables` property is a dictionary of variables that apply to
    this measurement.
    """
    def __init__(self, pd, measurement=None, groupidx=None, validx=None):
        self.__pd = pd
        self.reset(measurement, groupidx, validx)

    def reset(self, measurement, groupidx, validx=None):
        """
        Reset the measurement to the provided indices and measurement
        dictionary.
        """
        self.__measurement = measurement
        self.__groupidx = groupidx
        self.__validx = validx

    def filter(
            self, metricfilter=None, metric=None, variables=None, regex=None,
            function=None):
        """
        Given a filter as a metric name and a dictionary of variables,
        return True if this measurement passes the filter, and False
        otherwise.
        """
        if metricfilter:
            if metricfilter.startswith(self.__pd.name):
                metricfilter = metricfilter[len(self.__pd.name) + 1:]
            metric, variables = Measurement.parse_filter(metricfilter)
        if metric and self.metric != metric:
            return False
        if variables:
            mvariables = self.variables
            for k, v in six.iteritems(variables):
                if k not in mvariables or mvariables[k] != v:
                    return False
        if regex:
            mf = self.get_filter()
            if not re.search(regex, mf):
                return False
        if function:
            if not function(self):
                return False

        return True

    def get_filter(self, only_variables=[], ignoring_variables=[]):
        """
        Get the metric filter that identifies this measurement.  All variables
        are included by default.  If only a few variables should be included,
        provide them as list in `only_variables`.  If all variables except some
        should be included, do the same with `ignoring_variables`.
        """
        filt = '{}.{}'.format(self.__pd.name, self.metric)
        var_str = self.get_variables_str(
                only_variables=only_variables,
                ignoring_variables=ignoring_variables)
        if var_str:
            filt += ',' + var_str

        return filt

    @staticmethod
    def parse_filter(filter):
        vars = filter.split(',')
        metric = vars.pop(0)
        variables = dict()
        for var in vars:
            components = var.split('=')
            try:
                variables[components[0]] = int(components[1])
            except ValueError:
                variables[components[0]] = components[1]
        return metric, variables

    def get_variables_str(self, only_variables=[], ignoring_variables=[]):
        vars = []
        for var, val in six.iteritems(self.variables):
            if var in ignoring_variables:
                continue
            if only_variables and var not in only_variables:
                continue

            if isinstance(val, numbers.Number) and int(val) == val:
                val = int(val)

            vars.append((var, val))

        if not vars:
            return None

        vars.sort(key=lambda e: e[0])

        return ','.join(['{}={}'.format(var, val) for (var, val) in vars])

    def get_speedup(self, baseline):
        val = self.median or self.mean
        if val is None:
            return None

        val_baseline = baseline.median or baseline.mean
        if val_baseline is None:
            return None

        if val == val_baseline:
            return 1

        if val == 0 or val_baseline == 0:
            return None

        speedup = (
                val / val_baseline if self.larger_better else
                val_baseline / val)

        return speedup

    @property
    def variables_str(self):
        """
        Get a string containing the variables and associated values of the
        measurement, separated by commas, or None if there are no variables.
        """
        return self.get_variables_str()

    @property
    def labels_str(self):
        """
        Get a string containing the labels and associated values of the
        measurement, separated by commas, or None if there are no labels.
        """
        if not self.labels:
            return None

        lbls = [(lbl, val) for lbl, val in six.iteritems(self.labels)].sort(
                key=lambda e: e[0])
        return ','.join(
                ['{}={}'.format(lbl, '|'.join(val)) for (lbl, val) in lbls])

    @property
    def group(self):
        """Get the group number of this measurement."""
        return self.__groupidx

    @property
    def metric(self):
        """Get the metric name of this measurement."""
        return self.__measurement['metric']

    @property
    def unit(self):
        """Get the units of this measurement."""
        return self.__measurement['unit']

    @property
    def larger_better(self):
        """Get whether larger values are better for this measurement."""
        return self.__measurement.get(
                'larger_better', self.__pd.larger_better)

    @property
    def _tags(self):
        return self.__measurement.get('tags', [])

    @property
    def summary(self):
        """Get whether the measurement represents a summary of the perfdata."""
        return 'summary' in self._tags

    @property
    def context(self):
        """
        Get whether the measurement provides context for other measurements in
        the perfdata.
        """
        return 'context' in self._tags

    @property
    def value(self):
        """
        Get the value of this measurement, or None if only statistics are
        provided.
        """
        try:
            if self.__validx is not None:
                return self.__measurement['values'][self.__validx]
            else:
                return self.__measurement['value']
        except KeyError:
            return None

    @property
    def statistics(self):
        """Get the statistics dictionary for this measurement."""
        if 'values' in self.__measurement or 'value' in self.__measurement:
            return None
        stats = dict()
        for stat in [
                'samples', 'mean', 'std_dev', 'median', 'minimum', 'maximum']:
            try:
                stats[stat] = self.__measurement[stat]
            except KeyError:
                pass
        return stats

    @property
    def percentiles(self):
        """Generate the percentiles of this measurement."""
        if 'percentiles' not in self.__measurement:
            return
        for percentile in self.__measurement['percentiles']:
            yield {'p': percentile['%'], 'value': percentile['value']}

    @property
    def histogram(self):
        """Generate the histogram buckets of this measurement."""
        if 'histogram' not in self.__measurement:
            return
        for bucket in self.__measurement['histogram']:
            yield {
                'lower_inclusive': bucket['>='],
                'count': bucket['count'],
                'label': bucket.get('label')
            }

    @property
    def mean(self):
        """Get the mean of the measurement."""
        return self.__measurement.get('mean', self.value)

    @property
    def std_dev(self):
        """Get the standard deviation of the measurement."""
        return self.__measurement.get(
                'std_dev', 0 if self.value is not None else None)

    @property
    def median(self):
        """Get the median of the measurement."""
        return self.__measurement.get('median', self.value)

    @property
    def samples(self):
        """Get the number of samples represented by this measurement."""
        return self.__measurement.get(
                'samples', 1 if self.value is not None else None)

    @property
    def minimum(self):
        """Get the minimum value represented by this measurement."""
        return self.__measurement.get('minimum', self.value)

    @property
    def maximum(self):
        """Get the maximum value represented by this measurement."""
        return self.__measurement.get('maximum', self.value)

    @property
    def range(self):
        """Get the range represented by this measurement."""
        min = self.minimum
        max = self.maximum
        if min is None or max is None:
            return None

        return max - min

    @property
    def normalized_range(self):
        range = self.range
        if range is None:
            return None
        if not self.minimum:
            return 0
        return range / self.minimum

    @property
    def coefficient_of_variation(self):
        """
        Get the coefficient of variation (i.e relative standard deviation)
        of the measurement.
        """
        mean = self.mean
        sd = self.std_dev
        if not self.mean or sd is None:
            return 0

        return (sd / abs(mean))

    relative_std_dev = coefficient_of_variation

    @property
    def variables(self):
        """Get the variables that apply to this measurement."""
        if 'variables_parsed' not in self.__measurement:
            vars = self.__measurement['variables'] if (
                    'variables' in self.__measurement) else dict()
            vars.update(self.__pd.variables)
            self.__measurement['variables_parsed'] = vars

        return self.__measurement['variables_parsed']

    @property
    def labels(self):
        """Get the labels that apply to this measurement."""
        if 'labels_parsed' not in self.__measurement:
            lbls = dict()
            for key, val in six.iteritems(
                    self.__measurement.get('labels', dict())):
                if key not in lbls:
                    lbls[key] = val if isinstance(val, set) else set([val])
                else:
                    lbls[key] = lbls[key].union(val)
            self.__measurement['labels_parsed'] = lbls

        return self.__measurement['labels_parsed']

    @property
    def aggregate_values(self):
        return self.__measurement.get('aggregate_values', None)

    def scale(self):
        """
        Scale the representative value with the unit's appropriate prefix.
        """
        val = self.value
        if val is None:
            val = self.mean
        if val is None:
            return None, None, None

        if self.unit in units.units:
            return Scaler.scale(val, self.unit)
        else:
            return val, self.unit, 1

    def __getattr__(self, attr):
        """
        Treat the measurement's variables as attributes.
        """
        try:
            return self.variables[attr]
        except KeyError:
            try:
                return self.labels[attr]
            except KeyError:
                raise AttributeError

    def __repr__(self):
        val = None
        try:
            for t in ['value', 'mean']:
                val = getattr(self, t)
                if val is not None:
                    break
        except AttributeError:
            val = "none"

        return '{}{} ({}): {}'.format(
                '{}: '.format(self.group) if self.group else '', self.metric,
                self.unit, val)

    def __lt__(self, other):
        if self.__pd.name == other.__pd.name:
            return self.metric < other.metric
        else:
            return self.__pd.name < other.__pd.name

    def __eq__(self, other):
        return (
                self.__pd.name == other.__pd.name and
                self.metric == other.metric)


class Unit():
    def __init__(self, custom):
        self.unitstring = custom

    def __str__(self):
        return '#' + self.unitstring

    def __unicode__(self):
        return '#' + self.unitstring


class BuiltInUnits():
    class BuiltInUnit(Unit):
        def __init__(self, unitstring):
            self.unitstring = unitstring

        def __unicode__(self):
            return self.unitstring

    def __init__(self):
        units = {}

        def declare(d, lname, sname, prefixes, valstr=None):
            prefixes += [('', '')]
            for lpfx, spfx in prefixes:
                if isinstance(spfx, tuple):
                    spfx = spfx[1]

                unitstring = spfx + (valstr or sname or lname)
                unit = BuiltInUnits.BuiltInUnit(unitstring)
                units[lpfx + lname] = unit
                if sname is not None:
                    units[spfx + sname] = unit

        si_prefixes_lt1 = [
                ('yocto', 'y'), ('zepto', 'z'), ('atto', 'a'), ('fempto', 'f'),
                ('pico', 'p'), ('nano', 'n'), ('micro', ('u', u'µ')),
                ('milli', 'm'), ('centi', 'c'), ('deci', 'd')]
        si_prefixes_gt1 = [
                ('deca', 'da'), ('hecto', 'h'), ('kilo', 'k'), ('mega', 'M'),
                ('giga', 'G'), ('tera', 'T'), ('peta', 'P'), ('exa', 'E'),
                ('zetta', 'Z'), ('yotta', 'Y')]
        si_prefixes = si_prefixes_lt1 + si_prefixes_gt1

        bin_prefixes = [
                ('kibi', 'Ki'), ('mebi', 'Mi'), ('gibi', 'Gi'), ('tebi', 'Ti'),
                ('pebi', 'Pi'), ('exbi', 'Ei'), ('zebi', 'Zi'), ('yobi', 'Yi')]

        declare(units, 'seconds', 's', si_prefixes_lt1)
        declare(units, 'hertz', 'Hz', si_prefixes)

        declare(units, 'bytes', 'B', si_prefixes_gt1 + bin_prefixes)
        declare(
                units, 'bytes_per_second', 'Bps',
                si_prefixes_gt1 + bin_prefixes)
        declare(units, 'bits', 'b', si_prefixes_gt1 + bin_prefixes)
        declare(
                units, 'bits_per_second', 'bps',
                si_prefixes_gt1 + bin_prefixes)

        declare(units, 'joules', 'J', si_prefixes)
        declare(units, 'watts', 'W', si_prefixes)
        declare(units, 'watt_hours', 'Wh', si_prefixes)

        declare(units, 'degrees_celsius', 'C', [], u'°C')
        declare(units, 'degrees_fahrenheit', 'F', [], u'°F')
        declare(units, 'degrees_kelvin', 'K', [], u'°K')

        declare(units, 'instructions', None, si_prefixes_gt1)
        declare(units, 'cycles', None, si_prefixes_gt1)

        declare(units, 'instructions_per_second', 'IPS', si_prefixes_gt1)
        declare(units, 'instructions_per_cycle', 'IPC', [])
        declare(units, 'cycles_per_instruction', 'CPI', [])
        declare(
                units, 'floating_point_operations_per_second', 'FLOPS',
                si_prefixes_gt1)

        declare(units, 'seconds_cpu', 'sCPU', si_prefixes_lt1)
        declare(units, 'percent_cpu', '%CPU', [])
        declare(units, 'percent_cpus', '%CPUs', [])

        declare(units, 'frames_per_second', 'FPS', [])

        self.units = units

    builtins = {}

    def __getattr__(self, name):
        return self.units[name]


units = BuiltInUnits()


class AllowedTags():
    def __init__(self):
        pass

    def __getattr__(self, name):
        if name in ['summary', 'context']:
            return name
        raise AttributeError


tags = AllowedTags()


class Writer():
    """
    Writers write perfdata that's been added to the object.

    For example::

        wr = perfdata.Writer.new('myproject.mytest', 1)
        wr.add_value('mymetric', perfdata.units.nanoseconds, 12345)
        with wr.new_group() as grp:
            grp.add_value('anothermetric', perfdata.units.kibibytes, 98765)
        wr.write('test.pdj')
    """
    def __init__(self, name, version):
        self.name = name
        self.version = version
        self.data = []
        self.defaultgroup = []
        self.groups = []
        self.start_date = datetime.datetime.utcnow()
        self.configuration = None
        self.primary_metric = None

    def set_configuration(self, config):
        valid_config_keys = {
            'device_type': (str, unicode),
            'product_type': (str, unicode),
            'cpu_type': numbers.Number,
            'cpu_subtype': numbers.Number,
            'cpu_family': numbers.Number,
            'cpu_brand_string': (str, unicode),
            'logical_cpus': numbers.Number,
            'physical_cpus': numbers.Number,
            'memory_size': numbers.Number,
            'hw_pagesize': numbers.Number,
            'vm_pagesize': numbers.Number,
            'vm_pages': numbers.Number,
            'kernel_version': (str, unicode),
            'build': (str, unicode),
            'release_type': (str, unicode),
            'thread_groups_supported': bool,
            'boot_args': (str, unicode),
            'device_uuid': (str, unicode),
            'rootvol_size': numbers.Number,
            'rootvol_ssd': bool,
            'rootvol_fusion': bool,
            'dsc_overriding_images_present': bool,
            'dsc_optimized': bool,
            'max_battery_resistance_milliohms': numbers.Number,
            'max_vnodes': numbers.Number,
        }
        if not isinstance(config, dict):
            raise ValueError('configuration must be a dictionary')

        for k, v in six.iteritems(config.copy()):
            expected_type = valid_config_keys.get(k, None)
            if not expected_type or not isinstance(v, expected_type):
                del config[k]

        self.configuration = config

    def set_primary_metric(self, primary_metric):
        if not isinstance(primary_metric, (str, unicode)):
            raise ValueError('primary metric must be a string')

        self.primary_metric = primary_metric

    class Group():
        def __init__(self):
            self.measurements = []

        def __enter__(self, *args):
            return self

        def __exit__(self, *args):
            pass

        def __iter__(self):
            for msrmt in self.measurements:
                yield msrmt

        def _get_measurement(
                self, metric, unit, variables, labels, tags, larger_better):
            if not isinstance(metric, (str, unicode)):
                raise ValueError('metric must be a string')
            if not isinstance(unit, Unit):
                raise ValueError('unit must be a Unit')

            for desc, assoc in ('variables', variables), ('labels', labels):
                if assoc is not None:
                    if not isinstance(assoc, dict):
                        raise ValueError(desc + ' must be a dictionary')
                    for k, v in six.iteritems(assoc):
                        if not isinstance(k, (str, unicode)):
                            raise ValueError(
                                    '{} key {} not a string'.format(
                                            d=desc, k=k))
                        if not isinstance(v, (str, unicode, numbers.Number)):
                            raise ValueError((
                                    '{d} value {v} (for key {k}) must be '
                                    'a string or number').format(
                                    d=desc, k=k, v=v))

            mdict = {
                'metric': metric,
                'unit': unicode(unit),
            }
            if variables is not None:
                mdict['variables'] = variables
            if labels is not None:
                mdict['labels'] = labels
            if tags is not None:
                mdict['tags'] = tags
            if larger_better:
                mdict['larger_better'] = True

            return mdict

        def add_value(
                self, metric, unit, value, larger_better=False,
                variables=None, labels=None, tags=None):
            if not isinstance(value, numbers.Number):
                raise ValueError('value must be a number')

            if tags:
                if not isinstance(tags, Iterable):
                    raise ValueError('tags must be iterable')
                for tag in tags:
                    if tag not in ['summary', 'context']:
                        raise ValueError('invalid tag: ' + tag)

            mdict = self._get_measurement(
                    metric, unit, variables, labels, tags, larger_better)
            mdict['value'] = value

            self.measurements.append(mdict)

    def new_group(self):
        grp = Writer.Group()
        self.groups.append(grp)
        return grp

    def add_value(
            self, metric, unit, value, larger_better=False, variables=None,
            labels=None, tags=None):
        if not self.defaultgroup:
            self.defaultgroup = self.new_group()
        self.defaultgroup.add_value(
                metric, unit, value, larger_better, variables=variables,
                labels=labels, tags=tags)

    def __iter__(self):
        pd = {
            '$schema': 'https://perfdata.apple.com/schema/draft-02.json',
            'name': self.name,
            'version': self.version,
            'generator': 'perfdata.py-{}'.format(__version__),
            'start_date': self.start_date.isoformat(str('T')) + 'Z',
            'end_date': datetime.datetime.utcnow().isoformat(str('T')) + 'Z'
        }
        if self.configuration:
            pd['configuration'] = self.configuration
        if self.primary_metric:
            pd['primary_metric'] = self.primary_metric

        pd['data'] = list([list(grp) for grp in self.groups])

        for k, v in six.iteritems(pd):
            yield k, v

    def __unicode__(self):
        return json.dumps(dict(self), ensure_ascii=False)

    def __str__(self):
        return json.dumps(dict(self), ensure_ascii=True)

    def write_file(self, f):
        f.write(unicode(self))

    def write(self, out):
        if out is None:
            return unicode(self)
        if isinstance(out, (str, unicode)):
            with open(out, 'w') as outf:
                self.write_file(outf)
        elif isinstance(out, io.IOBase):
            self.write_file(out)


class ScaleSI():
    PREFIXES = {
        -24: ('y', 'yocto'), -21: ('z', 'zepto'), -18: ('a', 'atto'),
        -15: ('f', 'fempto'), -12: ('p', 'pico'), -9: ('n', 'nano'),
        -6: (u'µ', 'micro'), -3: ('m', 'milli'), 0: ('', ''),
        3: ('k', 'kilo'), 6: ('M', 'mega'), 9: ('G', 'giga'),
        12: ('T', 'tera'), 15: ('P', 'peta'), 18: ('E', 'exa'),
        21: ('Z', 'zetta'), 24: ('Y', 'yotta'),
    }

    def __init__(self, min=-24, max=24, full=False):
        self.min = min
        self.max = max
        self.full = full

    def __call__(self, value, prefix):
        orig_pow10 = 0
        for pow10, prefixes in six.iteritems(ScaleSI.PREFIXES):
            if prefix == prefixes[0] or prefix == prefixes[1]:
                orig_pow10 = pow10
                break
        else:
            return (value, prefix, 1)

        pow10 = orig_pow10
        while value < 1 and pow10 > self.min:
            pow10 -= 3
            value *= 1e3
        while value > 999 and pow10 < self.max:
            pow10 += 3
            value *= 1e-3

        scale = 10**(orig_pow10 - pow10)
        prefix = ScaleSI.PREFIXES[pow10][1 if self.full else 0]

        return (value, prefix, scale)


class ScaleBin():
    PREFIXES = {
        0: ('', ''), 10: ('Ki', 'kibi'), 20: ('Mi', 'mibi'),
        30: ('Gi', 'gibi'), 40: ('Ti', 'tebi'), 50: ('Pi', 'pebi'),
        60: ('Ei', 'exbi'), 70: ('Zi', 'zebi'), 80: ('Yi', 'yobi'),
    }

    def __call__(self, value, prefix):
        orig_pow2 = 0
        for pow2, prefixes in six.iteritems(ScaleBin.PREFIXES):
            if prefix == prefixes[0] or prefix == prefixes[1]:
                orig_pow2 = pow2
                break
        else:
            return (value, prefix, 1)

        pow2 = orig_pow2
        while value <= (1 + (1 / 1024)) and pow2 > 0:
            pow2 -= 10
            value *= 1024
        while value >= 1024 and pow2 < 80:
            pow2 += 10
            value /= 1024

        scale = 2**(orig_pow2 - pow2)
        prefix = ScaleBin.PREFIXES[pow2][0]

        return (value, prefix, scale)


class Scaler():
    SCALE_SI_ALL = ScaleSI()
    SCALE_SI_LE0 = ScaleSI(-24, 0)
    SCALE_SI_GE0 = ScaleSI(-24, 24)
    SCALE_SI_GE0_FULL = ScaleSI(-24, 24, full=True)
    SCALE_BIN_GE0 = ScaleBin()

    SCALES = [
        ('Hz', SCALE_SI_ALL), ('J', SCALE_SI_ALL), ('W', SCALE_SI_ALL),
        ('Wh', SCALE_SI_ALL),

        ('s', SCALE_SI_LE0), ('sCPU', SCALE_SI_LE0),

        ('instructions', SCALE_SI_GE0_FULL), ('cycles', SCALE_SI_GE0_FULL),

        ('IPC', SCALE_SI_GE0), ('FLOPS', SCALE_SI_GE0), ('IPS', SCALE_SI_GE0),
        ('CPI', SCALE_SI_GE0),

        ('B', SCALE_BIN_GE0), ('b', SCALE_BIN_GE0), ('Bps', SCALE_BIN_GE0),
        ('bps', SCALE_BIN_GE0),
    ]

    # Sort the scales from longest to shortest.  This lets the endswith
    # search find "instructions", and not just "s".
    SCALES.sort(key=lambda k: len(k[0]), reverse=True)

    @staticmethod
    def scale(value, unit):
        '''
        Given a value and a unit string, return a scaled value, the re-scaled
        unit, and the scaling factor used.
        '''
        # XXX Only scale standard units we know about.
        for base, scaler in Scaler.SCALES:
            if unit.endswith(base):
                prefix = unit[:(len(unit) - len(base))]
                scaled_value, prefix, scale = scaler(value, prefix)
                return scaled_value, prefix + base, scale

        return value, unit, 1


class Stats():
    class Sum():
        def __init__(self):
            self.sum = 0
            self.comp = 0

        def add(self, value):
            value -= self.comp

            tmp = self.sum + value
            self.comp = (tmp - self.sum) - value
            self.sum = tmp

    class Median():
        def __init__(self):
            self.values = []

        def update(self, value):
            self.values.append(value)

        def get_stats(self):
            self.values.sort()
            return self.values[(len(self.values) // 2)]

    class RunningValue():
        def __init__(self):
            self.values = []

        def update(self, value):
            self.values.append(value)

        def get_values(self):
            return self.values

    class PooledMean():
        def __init__(self):
            self.mean_weighted = Stats.Sum()
            self.variance_weighted = Stats.Sum()
            self.samples = 0
            self.count = 0

        def update(self, mean, std_dev, samples):
            self.count += 1
            self.samples += samples

            self.mean_weighted.add(samples * mean)
            self.variance_weighted.add((samples - 1) * (std_dev * std_dev))

        def get_stats(self):
            if self.samples == 0:
                return None, None, 0

            mean = self.mean_weighted.sum / self.samples
            std_dev = math.sqrt(
                    self.variance_weighted.sum / (self.samples / self.count))
            samples = self.samples

            return mean, std_dev, samples

    class RunningRange():
        def __init__(self):
            self.min = sys.float_info.max
            self.max = -sys.float_info.min

        def update(self, value):
            if value < self.min:
                self.min = value
            if value > self.max:
                self.max = value

        def get_stats(self):
            return self.min, self.max

    class RunningMean():
        def __init__(self):
            self.mean = 0
            self.S = 0
            self.samples = 0

        def update(self, value):
            samples = self.samples + 1
            mean = self.mean
            delta = value - mean
            mean = mean + delta / samples
            delta2 = value - mean

            self.S += delta * delta2
            self.mean = mean
            self.samples = samples

        def get_stats(self):
            if self.samples == 0:
                return None, None, 0
            if self.samples < 2:
                return self.mean, 0, self.samples
            mean = self.mean
            if self.samples > 1:
                std_dev = math.sqrt(self.S / (self.samples - 1))
            else:
                std_dev = 0
            samples = self.samples
            return mean, std_dev, samples

    def __init__(self, save_values=False):
        self.stats_mean = Stats.PooledMean()
        self.value_mean = Stats.RunningMean()
        self.median = Stats.Median()
        self.range = Stats.RunningRange()
        self.saw_stats = False
        self.median_value = None
        self.values_count = 0
        self.stats_count = 0
        self.values = [] if save_values else None

    def update(self, value):
        self.value_mean.update(value)
        if not self.saw_stats:
            self.median.update(value)
        self.range.update(value)
        if self.values is not None:
            self.values.append(value)
        self.values_count += 1

    def update_stats(self, mean, std_dev, samples, median):
        self.stats_mean.update(mean, std_dev, samples)
        self.range.update(mean)
        self.median_value = median
        self.stats_count += 1
        self.saw_stats = True

    @staticmethod
    def get_difference_description(a, b):
        a_mean = a.mean
        b_mean = b.mean

        magnitude = 1
        if a_mean != 0:
            magnitude = 1 - (b_mean / a_mean)

        better = a.larger_better
        amount = magnitude
        if a.larger_better:
            amount = -amount
        if magnitude > 0:
            better = not better

    def get_stats(self):
        vmean, vstd_dev, vsamples = self.value_mean.get_stats()
        pmean, pstd_dev, psamples = self.stats_mean.get_stats()
        median = None if self.saw_stats else self.median.get_stats()
        if self.stats_count == 1 and self.values_count == 0:
            median = self.median_value
        min, max = self.range.get_stats()

        if psamples == 0:
            return vmean, vstd_dev, vsamples, median, min, max
        if vsamples == 0:
            return pmean, pstd_dev, psamples, median, None, None

        all_stats = Stats.PooledMean()
        all_stats.update(pmean, pstd_dev, psamples)
        all_stats.update(vmean, vstd_dev, vsamples)

        mean, std_dev, samples = all_stats.get_stats()
        return mean, std_dev, samples, median, min, max

    def get_values(self):
        return self.values if not self.saw_stats else None


@functools.total_ordering
class Build():
    def __init__(self, desc):
        self.desc = desc
        m = re.match(
                r'(?P<major>\d+)(?P<minor>[A-Z])(?P<build>\d+)(?P<rev>[a-z])?',
                desc)
        self.major, self.minor, self.build, self.rev = m.group(
                'major', 'minor', 'build', 'rev')

    def __lt__(self, other):
        return (self.major < other.major or self.minor < other.minor or
                self.build < other.build or self.rev < other.rev)

    def __eq__(self, other):
        return (self.major == other.major or self.minor == other.minor or
                self.build == other.build or self.rev == other.rev)

    def __repr__(self):
        return self.desc


class Keychain():
    available = True

    @staticmethod
    def get_user_password(url):
        security_user_re = br'"acct"\<blob\>="(\w+)"'
        security_password_re = br'password: "(.+)"'

        try:
            out = subprocess.check_output(
                    ['security', 'find-internet-password', '-g', '-s', url],
                    stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError:
            print('Keychain not available\n', file=sys.stderr)
            Keychain.available = False
            return None, None

        user_matches = re.search(security_user_re, out)
        if not user_matches:
            return None, None
        user = user_matches.group(1)

        password_matches = re.search(security_password_re, out)
        if not password_matches:
            raise RuntimeError('cannot parse password from security output')
        password = password_matches.group(1)

        return user, password

    @staticmethod
    def prompt(prefix, default=None, secure=False):
        if secure:
            return getpass.getpass(prefix + ': ') or default

        sys.stderr.write(prefix)
        if default:
            sys.stderr.write(' [{}]'.format(default))
        sys.stderr.write(': ')

        resp = raw_input()
        if not resp:
            resp = default
        return resp

    @staticmethod
    def prompt_user_password(url, user=None):
        url = re.sub(r'^https?\:\/\/', '', url)
        if not user:
            user, password = Keychain.get_user_password(url)
        if not user:
            print(
                'could not determine username from keychain', file=sys.stderr)
            user = Keychain.prompt('OD username', getpass.getuser())

        if not password:
            password = Keychain.prompt('Password', secure=True)
            print(
                    "If you have a keychain, add this password with `security "
                    "add-internet-password -U -s {} -a {} -w'\n".format(
                            url, user), file=sys.stderr)

        return user, password

    @staticmethod
    def handle_user_password(url, user=None, password=None):
        if user and password:
            return user, password
        return Keychain.prompt_user_password(url, user=user)


class NetworkSource():
    @staticmethod
    def import_requests():
        """
        Get requests, by hook or by crook.
        """
        try:
            import requests
            from requests.packages.urllib3.exceptions import (
                    InsecureRequestWarning)
            from requests.packages.urllib3 import disable_warnings
        except ImportError:
            # Try to break some weird import dependency in pip.  What we're
            # doing here isn't exactly kosher so we'll just try our best and
            # let the import statements in the next stanza fail if they do.
            try:
                from pip._internal.compat import WINDOWS
                WINDOWS
            except ImportError:
                pass

            import pip._vendor.requests as requests
            from pip._vendor.requests.packages.urllib3.exceptions import (
                    InsecureRequestWarning)
            from pip._vendor.requests.packages.urllib3 import disable_warnings
        disable_warnings(InsecureRequestWarning)
        return requests

    session_cache = {}

    @staticmethod
    def get_session(id, url, user=None, password=None):
        """
        Return a requests session that clients in this file can use for
        accessing servers.
        """
        requests = NetworkSource.import_requests()

        if id in NetworkSource.session_cache:
            return NetworkSource.session_cache[id]

        session = requests.Session()
        session.auth = Keychain.handle_user_password(
                url, user=user, password=password)
        session.headers = {
            'User-Agent': 'perfdata.py v' + __version__
        }
        NetworkSource.session_cache[id] = session
        session.verify = enable_cert_verify
        return session


class Splunk():
    """
    Search Splunk for perfdata.
    """
    def __init__(
            self, base_url, index, user=None, password=None,
            devices=[], verbose=0, lookback_days=30, data_key='',
            extra_query='', timeout_secs=30):
        """
        Create a new Splunk searcher.

        base_url -- the URL of the Splunk instance to target
        index -- the Splunk index to search in
        user -- the user account name to authenticate with
        password -- the user account password to authenticate with
        devices -- a list of devices to constrain the search to
        verbose -- amount of logging (0 is none)
        lookback_days -- how many days in the past to include in the search
        data_key -- a prefix for the data fields
        extra_query -- any additional query components for the search string
        timeout_secs -- how long to wait for results from Splunk, in seconds
        """
        self.base_url = base_url
        self.index = index
        self.extra_query = extra_query
        self.jobs_api_url = self.base_url + '/services/search/jobs'
        self.verbose = verbose
        self.lookback_days = lookback_days
        self.timeout_secs = timeout_secs
        self.data_key = data_key
        self.data_prefix = data_key + '.' if data_key else ''

        self.session = NetworkSource.get_session(
                'od', self.base_url, user=user, password=password)

    def debug(self, msg, **kwargs):
        if self.verbose > 1:
            print(
                    str(datetime.datetime.now().time()) + ': SPLUNK ' + msg,
                    **kwargs)

    def get(self, url, timeout=None, **kwargs):
        self.debug('get {}'.format(url))
        timeout = timeout if timeout is not None else 120
        res = self.session.get(url, timeout=timeout, **kwargs)
        return res

    def post(self, url, timeout=None, **kwargs):
        self.debug('post {}'.format(url))
        timeout = timeout if timeout is not None else 120
        res = self.session.post(url, timeout=timeout, **kwargs)
        return res

    @staticmethod
    def sanitize_device_name(device):
        return re.sub(r'ap$', '', device.lower())

    @staticmethod
    def iter_perfdatas_in_result(result):
        """
        Iterate through the perfdata's found in a given BATS result dictionary.
        """
        if 'perfdata' in result:
            for pd in result['perfdata']:
                # Smooth over all manner of sins.
                if 'configuration' not in pd:
                    pd['configuration'] = {}
                if 'build' not in pd['configuration']:
                    pd['configuration']['build'] = result['build']
                if 'device_type' not in pd['configuration']:
                    device = Splunk.sanitize_device_name(
                            result['device_model'])
                    pd['configuration']['device_type'] = device.upper()
                yield pd
        elif 'PerformanceData' in result:
            for pd in result['PerformanceData']:
                if 'configuration' not in pd:
                    pd['configuration'] = {}
                pd['configuration']['build'] = result['build']
                pd['configuration']['device_type'] = (
                        Splunk.sanitize_device_name(
                            result['device_model']).upper())
                yield pd

    def get_perfdatas_for_search(self, search_str):
        """
        Iterate through the perfdatas found by executing a search.

        search_str -- the raw search query to submit to Splunk
        """
        form = {
            'search': search_str,
            'output_mode': 'json',
            'earliest_time': '-{}d'.format(self.lookback_days),
            'latest_time': 'now',
            'preview': False,
        }

        self.debug('search {}'.format(form))

        requests = NetworkSource.import_requests()
        try:
            res = self.post(self.jobs_api_url, data=form)
        except requests.exceptions.ConnectionError as e:
            print(
                    'error: cannot connect to Splunk: ' + str(e),
                    file=sys.stderr)
            return

        try:
            res.raise_for_status()
        except IOError as e:
            print(
                    'error: request to Splunk failed: ' + str(e),
                    file=sys.stderr)
            return

        searchid = res.json()['sid']

        # Splunk won't be ready by the time we try to grab the results of the
        # search.  Wait half-a-second to start, and then back off exponentially
        # if we still don't see results, with up to 4 seconds of backoff.
        backoff_secs = 0.5
        duration_secs = 0

        while duration_secs < self.timeout_secs:
            time.sleep(backoff_secs)
            duration_secs += backoff_secs

            # Only backoff to every 4 seconds.
            if backoff_secs < 4:
                backoff_secs *= 2

            results_url = '{base}/{searchid}/results'.format(
                    base=self.jobs_api_url, searchid=searchid)
            res = self.get(results_url, params={
                'output_mode': 'json',
                'count': 10000
            })

            if res.status_code == 403 or res.status_code == 401:
                print(
                        'permission to Splunk ({}) denied'.format(
                                self.base_url), file=sys.stderr)
                return

            try:
                res.raise_for_status()
            except IOError as e:
                print('error: request to Splunk failed: ' + e, file=sys.stderr)
                return

            if res.status_code == 200:
                break

        if res.status_code != 200:
            print('error: Splunk ({}) never returned results'.format(
                    self.base_url), file=sys.stderr)
            return

        results = res.json()['results']
        self.debug('result {} results'.format(len(results)))
        for result in results:
            rawjson = result['_raw']

            data = json.loads(rawjson)
            if self.data_key:
                data = data[self.data_key]
            for pd in Splunk.iter_perfdatas_in_result(data):
                yield pd

    def get_search_command(
                self, devices=None, projects=None, test_names=None, build=None,
                baselines_only=False):
        """
        Return the search string for the given query parameters.

        devices -- limit the search to some devices
        projects -- limit the search to some projects
        test_names -- limit the search to some BATS test IDs
        build -- limit the search to a particular OS build
        baselines_only -- limit the search to baseline data
        """
        search_str = 'search index={index} {extra}'.format(
                index=self.index, extra=self.extra_query)
        pfx = self.data_prefix

        search_str += (
                '({}perfdata{{}}.name::* OR '.format(pfx) +
                '{}PerformanceData{{}}.bats_test_name::*)'.format(pfx))

        if devices:
            search_str += ' ('
            search_str += ' OR '.join(
                    ['{}BATS-Device::{}'.format(pfx, d) for d in devices])
            search_str += ')'
        if test_names:
            search_str += ' ('
            search_str += ' OR '.join(
                    ['{}test_id::{}'.format(pfx, n) for n in test_names])
            search_str += ')'
        if projects:
            search_str += ' ('
            search_str += ' OR '.join([
                    '{}perfdata{{}}.name={}.*'.format(pfx, p) for p in
                    projects])
            search_str += ')'
        if build:
            search_str += ' {}build::{}'.format(pfx, build)
        if baselines_only:
            search_str += ' {}is_baseline::true'.format(pfx)

        return search_str

    def get_perfdatas_for_container(
            self, container_id, devices=None, test_names=None, projects=None):
        search_str = self.get_search_command(
                devices=devices, test_names=test_names, projects=projects)
        search_str += ' {}container_id::{}'.format(
                self.data_prefix, container_id)

        for pd in self.get_perfdatas_for_search(search_str):
            yield pd

    def get_perfdatas_for_test_plan(
            self, test_plan_id, devices=None, test_names=None, projects=None):
        search_str = self.get_search_command(
                devices=devices, test_names=test_names, projects=projects)
        search_str += ' {}BATS-ID={test_plan_id}'.format(
                self.data_prefix, test_plan_id=test_plan_id)

        for pd in self.get_perfdatas_for_search(search_str):
            yield pd

    def get_perfdatas_for_test_names(
            self, test_names, devices=None, build=None,
            baselines_only=False, projects=None):
        search_str = self.get_search_command(
                devices=devices, test_names=test_names, projects=projects,
                build=build, baselines_only=baselines_only)
        for pd in self.get_perfdatas_for_search(search_str):
            yield pd

    def get_perfdatas_for_result_uuid(self, result_uuid):
        search_str = self.get_search_command()
        search_str += ' uuid="{}"'.format(result_uuid)
        for pd in self.get_perfdatas_for_search(search_str):
            yield pd

    def get_perfdatas_for_baselines(
            self, builds, names, projects=None, devices=None):
        pds = []
        for build in builds:
            for pd in self.get_perfdatas_for_test_names(
                    names, build=build, projects=projects,
                    baselines_only=True, devices=devices):
                pds.append(pd)
        return pds


class Loader():
    def __init__(
            self, devices=[], test_names=[], projects=[], cache=None,
            verbose=0, quiet=False, debug=False, disable_network=False,
            timeout_secs=30):
        self.devices = devices
        self.test_names = test_names
        self.projects = projects
        self.verbose = verbose
        self.timeout_secs = timeout_secs
        self.cache = cache
        if self.cache:
            try:
                os.mkdir(self.cache)
            except OSError:
                pass
        self.quiet = quiet
        self.splunks = []
        self.disable_network = disable_network

    @staticmethod
    def load_json_file_array(file):
        pd = None
        try:
            pd = json.load(file)
        except json.decoder.JSONDecodeError as e:
            raise ValidationError('invalid JSON in {}: {}'.format(
                    file.name, e))
        if isinstance(pd, list):
            return pd
        elif isinstance(pd, dict):
            return [pd]
        else:
            raise ValidationError(
                    'JSON must be object or array in ' + file.name)

    @staticmethod
    def load_json_path_array(path):
        with open(path) as file:
            return Loader.load_json_file_array(file)

    def log(self, msg):
        if not self.quiet:
            print(msg, file=sys.stderr)

    def get_cache_path(self, id_type, *args):
        id = id_type + '_' + '-'.join(args) + '.pdj'
        return os.path.join(self.cache, id)

    def found_in_cache(self, id_type, *args):
        return os.path.isfile(self.get_cache_path(id_type, *args))

    def get_cache(self, id_type, *args):
        path = self.get_cache_path(id_type, *args)
        self.log('loading perfdata from cache at ' + path)
        pds = self.load_json_path_array(path)
        for pd in pds:
            yield pd

    def set_cache(self, pds, id_type, *args):
        path = self.get_cache_path(id_type, *args)
        self.log('caching perfdata in ' + path)
        with open(path, 'w') as file:
            file.write(json.dumps([pd for pd in pds]))

    def cache_generator(id_type):
        def cacher(func):
            @functools.wraps(func)
            def wrapper(self, *args, **kwargs):
                if not self.cache:
                    for item in func(self, *args, **kwargs):
                        yield item
                    return

                if self.found_in_cache(id_type, *args):
                    for pd in self.get_cache(id_type, *args):
                        yield pd
                    return

                caching = []
                for item in func(self, *args, **kwargs):
                    caching.append(item)
                    yield item

                if caching:
                    self.set_cache(caching, id_type, *args)
            return wrapper
        return cacher

    @cache_generator('bats-container')
    def get_perfdatas_for_container(self, container_id):
        if self.disable_network:
            return

        for splunk in self.splunks:
            self.log('requesting container {} perfdata from {}'.format(
                    container_id, splunk.base_url))
            for pd in splunk.get_perfdatas_for_container(
                    container_id, devices=self.devices,
                    test_names=self.test_names, projects=self.projects):
                yield pd

    @cache_generator('baselines')
    def get_perfdatas_for_baselines(self, builds, names):
        if self.disable_network:
            return

        for splunk in self.splunks:
            self.log('requesting {} baseline perfdata from {}'.format(
                    builds, splunk.base_url))
            for pd in splunk.get_perfdatas_for_baselines(
                    builds, names, devices=self.devices,
                    projects=self.projects):
                yield pd

    @cache_generator('bats-test-plan')
    def get_perfdatas_for_test_plan(self, test_plan_id=None):
        if self.disable_network:
            return

        for splunk in self.splunks:
            self.log('requesting test plan {} perfdata from {}'.format(
                    test_plan_id, splunk.base_url))
            for pd in splunk.get_perfdatas_for_test_plan(
                    test_plan_id, devices=self.devices,
                    test_names=self.test_names, projects=self.projects):
                yield pd

    @cache_generator('bats-result')
    def get_perfdatas_for_result_uuid(self, result_uuid):
        if self.disable_network:
            return

        for splunk in self.splunks:
            self.log('requesting result {} perfdata from {}'.format(
                    result_uuid, splunk.base_url))
            for pd in splunk.get_perfdatas_for_result_uuid(result_uuid):
                yield pd

    def init_network_sources(self):
        if self.disable_network:
            return

        if not self.splunks:
            self.splunks.append(Splunk(
                    base_url='https://splunk.iso.apple.com:8089',
                    index='results-service', devices=self.devices,
                    verbose=self.verbose, extra_query='BATS-ID::* ',
                    timeout_secs=self.timeout_secs))

    def allowed_perfdata(self, pd):
        if not self.devices:
            return True

        for device in self.devices:
            if re.search(device, pd['configuration']['device_type']):
                return True

        return False

    def iterate_perfdata(self, source):
        if source == '-':
            for pd in Loader.load_json_file_array(sys.stdin):
                yield pd
            return

        if os.path.isdir(source):
            for root, subdirs, files in os.walk(source):
                for filename in files:
                    if not filename.endswith(('.pdj', '.perfdata')):
                        continue
                    for pd in Loader.load_json_path_array(
                            os.path.join(root, filename)):
                        if self.allowed_perfdata(pd):
                            yield pd
            return

        if os.path.isfile(source):
            if zipfile.is_zipfile(source):
                with zipfile.ZipFile(source) as z:
                    for filename in z.namelist():
                        if os.path.isdir(filename) or not filename.endswith((
                                '.pdj', '.perfdata')):
                            continue
                        with z.open(filename) as file:
                            for pd in Loader.load_json_file_array(file):
                                if self.allowed_perfdata(pd):
                                    yield pd
                return

            if tarfile.is_tarfile(source):
                with tarfile.open(source) as tfile:
                    for info in tfile:
                        if info.isdir() or not info.name.endswith((
                                '.pdj', '.perfdata')):
                            continue
                        file = tfile.extractfile(info.name)
                        for pd in Loader.load_json_file_array(file):
                            if self.allowed_perfdata(pd):
                                yield pd
                        file.close()
                return

            for pd in Loader.load_json_path_array(source):
                if self.allowed_perfdata(pd):
                    yield pd
            return

        def match_any(regexes, haystack):
            for regex in regexes:
                m = re.match(regex, haystack)
                if m:
                    return m
            return None

        # BATS containers
        m = match_any([
                r'bats-container:(?P<id>\d+)',
                r'https://bats.apple.com/.+/?containers?/(?P<id>\d+)'], source)
        if m:
            self.init_network_sources()
            for pd in self.get_perfdatas_for_container(m.group('id')):
                yield pd
            return

        # BATS test plans
        m = match_any([
                r'bats-test-plan:(?P<id>\d+)',
                r'https://bats.apple.com/.*/?test-plan/(?P<id>\d+)'], source)
        if m:
            self.init_network_sources()
            for pd in self.get_perfdatas_for_test_plan(m.group('id')):
                yield pd
            return

        # BATS results by UUID
        m = match_any([r'bats-result:(?P<id>\d+)', (
                r'https://bats.apple.com/rdb/result/(?P<scope>[^/]+)' +
                r'/(?P<id>[\w-]+)')],
                source)
        if m:
            self.init_network_sources()
            scope = m.group('scope')
            # Splunk does not support non-default scopes.
            if scope != 'default' and scope != 'Production':
                raise RuntimeError(
                        'only the default scope is supported by Splunk')

            for splunk in self.splunks:
                for pd in splunk.get_perfdatas_for_result_uuid(
                        m.group('id'), scope=scope):
                    yield pd

        m = match_any([r'splunk-search:(?P<search>.+)'], source)
        if m:
            self.init_network_sources()
            for pd in self.get_perfdatas_for_splunk_search(m.group('search')):
                yield pd
            return

        m = match_any([r'build:(?P<build>.+)'], source)
        if m:
            if not self.test_names:
                raise RuntimeError('build sources require a test name')

            self.init_network_sources()
            for pd in self.get_perfdatas_for_baselines(
                    [m.group('build')], self.test_names):
                yield pd
            return

        raise RuntimeError('{}: cannot find source'.format(source))

    def get_perfdata(self, source):
        pds = []
        for pd in self.iterate_perfdata(source):
            pds.append(pd)
        return pds
