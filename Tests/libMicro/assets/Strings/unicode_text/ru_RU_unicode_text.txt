По своей природе компьютеры могут работать лишь с числами. И для того, чтобы они могли хранить в памяти буквы или другие символы, каждому такому символу должно быть поставлено в соответствие число. До того, как появился Unicode, в мире имели хождение сотни различных схем подобного кодирования символов. Но ни одна из этих схем не была столь универсальной, чтобы описать все необходимые символы: например, только для кодирования букв, входящих в алфавиты языков Европейского Сообщества, необходимо было использовать несколько различных кодировок. По большому счёту даже и для отдельного языка, скажем, английского, не существовало единой системы кодирования, включавшей в себя все обычно используемые буквы, знаки пунктуации и технические символы.

Более того, все эти схемы кодирования часто даже не были совместимы друг с другом. К примеру, две разные кодировки могли использовать один и тот же код для представления двух разных символов или присваивать разные коды одной и той же букве. В этой ситуации для любого компьютера, а особенно сервера, приходилось поддерживать несколько разных кодировок, которые могли понадобиться, но даже и тогда при передаче данных на другую платформу или при их преобразовании в другую кодировку всегда оставался риск, что эти данные окажутся повреждёнными.
